{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0",
   "metadata": {},
   "outputs": [],
   "source": [
    "canonical_csv_path = \"../../data/csv/fake_canonical.csv\"\n",
    "test_csv_path = \"../../data/csv/fake_genai.csv\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Load original and modified datasets\n",
    "canonical_csv = pd.read_csv(canonical_csv_path).sort_values(by=\"id\").reset_index(drop=True)\n",
    "test_csv = pd.read_csv(test_csv_path).sort_values(by=\"id\").reset_index(drop=True)\n",
    "\n",
    "# Sort by ID to align entries\n",
    "canonical_csv = canonical_csv.sort_values(by=\"id\").reset_index(drop=True)\n",
    "test_csv = test_csv.sort_values(by=\"id\").reset_index(drop=True)\n",
    "\n",
    "# Limit canonical to just the test's columns\n",
    "common_columns = test_csv.columns\n",
    "canonical_csv = canonical_csv[common_columns]\n",
    "\n",
    "print(f\"Comparing the following columns: {list(common_columns)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ML Model Output Evaluation Report\n",
    "\n",
    "# Define comparison function for strict match\n",
    "def compare_fields(val1, val2):\n",
    "    return str(val1).strip().lower() == str(val2).strip().lower()\n",
    "\n",
    "# Setup\n",
    "accuracy_report = {}\n",
    "examples = []\n",
    "total_rows = len(test_csv)\n",
    "comparison_columns = [col for col in common_columns if col != \"id\"]\n",
    "row_correct_counts = [0] * total_rows\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from IPython.display import display, HTML\n",
    "plt.rcParams['figure.figsize'] = [10, 5]\n",
    "sns.set_theme(style=\"whitegrid\")\n",
    "\n",
    "# Column-wise accuracy tracking\n",
    "total_correct = 0\n",
    "for column in comparison_columns:\n",
    "    correct = 0\n",
    "    diffs = []\n",
    "    for i in range(total_rows):\n",
    "        val1 = canonical_csv[column][i]\n",
    "        val2 = test_csv[column][i]\n",
    "        match = compare_fields(val1, val2)\n",
    "        if match:\n",
    "            correct += 1\n",
    "            row_correct_counts[i] += 1\n",
    "        elif len(diffs) < 5:\n",
    "            diffs.append({\n",
    "                \"id\": test_csv[\"id\"][i],\n",
    "                \"original\": \"\" if pd.isna(val1) else val1,\n",
    "                \"test\": \"\" if pd.isna(val2) else val2\n",
    "            })\n",
    "    total_correct += correct\n",
    "    accuracy_report[column] = {\n",
    "        \"correct\": correct,\n",
    "        \"total\": total_rows,\n",
    "        \"accuracy\": round(correct / total_rows, 3),\n",
    "        \"examples\": diffs\n",
    "    }\n",
    "\n",
    "# Total accuracy score\n",
    "total_fields = total_rows * len(comparison_columns)\n",
    "total_accuracy = round(total_correct / total_fields, 3)\n",
    "print(f\"\\n🧠 Model Accuracy Summary\")\n",
    "print(f\"✅ Total Accuracy Across All Fields: {total_accuracy * 100:.1f}%\\n\")\n",
    "\n",
    "# Accuracy per column\n",
    "summary_df = pd.DataFrame.from_dict({col: {\"accuracy\": v[\"accuracy\"]} for col, v in accuracy_report.items()}, orient=\"index\")\n",
    "print(\"📊 Per-Field Accuracy:\")\n",
    "display(summary_df)\n",
    "\n",
    "# Visual: Barplot of per-column accuracy\n",
    "plt.figure()\n",
    "sns.barplot(x=summary_df.index, y=summary_df[\"accuracy\"])\n",
    "plt.title(\"Model Accuracy by Field\")\n",
    "plt.ylabel(\"Accuracy\")\n",
    "plt.xlabel(\"Field\")\n",
    "plt.ylim(0, 1)\n",
    "plt.xticks(rotation=45)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Visual: Histogram of row accuracy\n",
    "row_accuracy = pd.DataFrame({\n",
    "    \"id\": test_csv[\"id\"],\n",
    "    \"correct_fields\": row_correct_counts,\n",
    "    \"total_fields\": len(comparison_columns),\n",
    "    \"row_accuracy\": [round(c / len(comparison_columns), 3) for c in row_correct_counts]\n",
    "})\n",
    "\n",
    "plt.figure()\n",
    "sns.histplot(row_accuracy[\"row_accuracy\"], bins=10, kde=True)\n",
    "plt.title(\"Distribution of Accuracy per Record\")\n",
    "plt.xlabel(\"Row Accuracy\")\n",
    "plt.ylabel(\"Number of Records\")\n",
    "plt.xlim(0, 1)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Detailed field-level diffs\n",
    "for col, data in accuracy_report.items():\n",
    "    if data[\"examples\"]:\n",
    "        print(f\"\\n❌ Sample Mismatches in Column: {col}\")\n",
    "        display(pd.DataFrame(data[\"examples\"]))\n",
    "\n",
    "\n",
    "\n",
    "# 🔍 Diff View of Test CSV\n",
    "from difflib import SequenceMatcher\n",
    "\n",
    "def similarity(a, b):\n",
    "    return SequenceMatcher(None, str(a).strip().lower(), str(b).strip().lower()).ratio()\n",
    "\n",
    "def highlight_diffs(row):\n",
    "    styled = []\n",
    "    for col in comparison_columns:\n",
    "        test_val = test_csv.at[row.name, col]\n",
    "        canon_val = canonical_csv.at[row.name, col]\n",
    "        sim = similarity(test_val, canon_val)\n",
    "        if sim == 1:\n",
    "            style = \"background-color: #e6ffe6\"  # greenish for match\n",
    "            display_val = test_val\n",
    "        else:\n",
    "            # yellow to red based on similarity\n",
    "            red = int((1 - sim) * 255)\n",
    "            color = f\"#{255:02x}{255 - red:02x}{128:02x}\"\n",
    "            style = f\"background-color: {color}\"\n",
    "            display_val = f\"{test_val}<br><small><i>→ {canon_val}</i></small>\"\n",
    "        styled.append(f'<td style=\"{style}\">{display_val}</td>')\n",
    "    return f'<tr><td>{row[\"id\"]}</td>' + ''.join(styled) + '</tr>'\n",
    "\n",
    "print(\"\\n🧾 Diff View of Model Output:\")\n",
    "html_table = '<table border=\"1\" style=\"border-collapse: collapse\"><tr><th>ID</th>' + ''.join([f'<th>{col}</th>' for col in comparison_columns]) + '</tr>'\n",
    "html_table += '\\n'.join(test_csv.apply(highlight_diffs, axis=1))\n",
    "html_table += '</table>'\n",
    "display(HTML(html_table))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Additional evaluation metrics\n",
    "# Calculate precision, recall, F1, similarity, and coverage for each field\n",
    "from difflib import SequenceMatcher\n",
    "import numpy as np\n",
    "\n",
    "metrics = {}\n",
    "for col in comparison_columns:\n",
    "    y_true = canonical_csv[col].fillna(\"\")\n",
    "    y_pred = test_csv[col].fillna(\"\")\n",
    "    # Count true/false positives/negatives\n",
    "    tp = (((y_true.str.lower() == y_pred.str.lower()) & (y_true != \"\")).sum())\n",
    "    fp = (((y_true.str.lower() != y_pred.str.lower()) & (y_pred != \"\")).sum())\n",
    "    fn = (((y_true != \"\") & (y_pred == \"\")).sum())\n",
    "    precision = tp / (tp + fp) if (tp + fp) else 0\n",
    "    recall = tp / (tp + fn) if (tp + fn) else 0\n",
    "    f1 = 2 * precision * recall / (precision + recall) if (precision + recall) else 0\n",
    "    # Character-level similarity and prediction coverage\n",
    "    similarity = np.mean([SequenceMatcher(None, str(a).lower(), str(b).lower()).ratio() for a, b in zip(y_true, y_pred)])\n",
    "    coverage = (y_pred != \"\").mean()\n",
    "    metrics[col] = {\n",
    "        \"precision\": round(precision, 3),\n",
    "        \"recall\": round(recall, 3),\n",
    "        \"f1\": round(f1, 3),\n",
    "        \"similarity\": round(similarity, 3),\n",
    "        \"coverage\": round(coverage, 3),\n",
    "    }\n",
    "\n",
    "# Macro averages across all fields\n",
    "macro_p = np.mean([m[\"precision\"] for m in metrics.values()])\n",
    "macro_r = np.mean([m[\"recall\"] for m in metrics.values()])\n",
    "macro_f1 = np.mean([m[\"f1\"] for m in metrics.values()])\n",
    "macro_sim = np.mean([m[\"similarity\"] for m in metrics.values()])\n",
    "macro_cov = np.mean([m[\"coverage\"] for m in metrics.values()])\n",
    "metrics[\"macro_avg\"] = {\n",
    "    \"precision\": round(macro_p, 3),\n",
    "    \"recall\": round(macro_r, 3),\n",
    "    \"f1\": round(macro_f1, 3),\n",
    "    \"similarity\": round(macro_sim, 3),\n",
    "    \"coverage\": round(macro_cov, 3),\n",
    "}\n",
    "metrics_df = pd.DataFrame(metrics).T\n",
    "print(\"\\n🔬 Precision, Recall, F1, Similarity, Coverage:\")\n",
    "display(metrics_df)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4",
   "metadata": {},
   "source": [
    "# Metric Definitions\n",
    "\n",
    "**Precision** measures how many of the model's non-empty answers were correct.\n",
    "\n",
    "**Recall** captures how many of the true answers the model successfully filled in.\n",
    "\n",
    "**F1** is a single score balancing precision and recall.\n",
    "\n",
    "**Similarity** looks at how close the predicted text is to the true text even if they aren't exactly the same.\n",
    "\n",
    "**Coverage** shows the percentage of records where the model attempted an answer at all.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "3.9.9",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
